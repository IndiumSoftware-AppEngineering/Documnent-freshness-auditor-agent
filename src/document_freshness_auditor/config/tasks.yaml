audit_task:
  description: >
    Audit documentation freshness for {project_path} by following these EXACT steps:
    
    1. DISCOVERY: Use 'list_files' to get a complete list of all files in the project.
    
    2. CHECKLIST: Create a mental or scratchpad checklist of all discovered files.
    
    3. ITERATIVE ANALYSIS (LOOP): For EACH file in your checklist, perform the appropriate checks, ALWAYS passing 'project_root={project_path}' to EVERY tool:
       - Python Files (.py): 
         * Run 'Docstring Signature Auditor' (passing project_root={project_path}).
         * Run 'Code Comment Auditor' (passing project_root={project_path}).
         * Run 'git_analyzer' (passing project_root={project_path}).
       - Markdown Files (.md):
         * Run 'README Structure Auditor' (passing project_root={project_path}) if it's a README.
         * Run 'SRS Parser' (passing path={project_path}) if it's an SRS.
       - API Specs (yaml/json):
         * Run 'API Implementation Auditor' (passing project_root={project_path}).
    
    4. THOROUGHNESS CHECK: Verify you have called at least one tool for EVERY file listed in step 1.
    
    5. REPORTING:
       - Assemble a JSON object containing a 'files' array.
       - Each entry in 'files' MUST include:
          * 'file': The normalized relative path.
          * 'metrics': A dictionary with these EXACT keys: 
               - 'total_functions': int
               - 'functions_with_docstrings': int
               - 'total_params': int
               - 'documented_params': int
               - 'critical_issues': int
               - 'major_issues': int
               - 'minor_issues': int
               - 'last_updated_iso': str (ISO-8601)
               - 'doc_type': str (inline_docstring, readme, api_spec, or srs)
           * 'issues': The list of findings (can be an empty list if no issues).
       - You MUST include an entry for EVERY single file found in the DISCOVERY step.
       - FINAL OUTPUT MUST BE PURE JSON. No conversation, no preamble.

    Output: JSON array with issues grouped by file, each with severity and description.
  expected_output: >
    JSON array of issues: [{"file": "path", "type": "docstring|readme|api|srs|comment", "line": N, "severity": "critical|major|minor", "description": "...", "metrics": {...}}]
  agent: documentation_auditor


freshness_scorer_task:
  description: >
    Calculate documentation freshness scores (0-100%) and confidence levels 
    EXCLUSIVELY using the 'freshness_scorer' tool for EVERY file identified 
    in the audit_task.
    
    STRICT INSTRUCTIONS:
    1. For each file in the audit output:
       a. Extract all available metrics from the audit task JSON for that file.
       b. Create a 'metrics' dictionary containing EXACTLY these keys:
          - doc_type (e.g., inline_docstring, readme, api_spec)
          - total_functions, functions_with_docstrings
          - total_params, documented_params
          - critical_issues, major_issues, minor_issues
          - last_updated_iso (if available from git_analyzer)
       c. CALL the 'freshness_scorer' tool with:
          - file_path: the file's path
          - metrics: the dictionary you created
       d. Capture the EXACT output from the tool.
    2. NEVER estimate any score or severity yourself.
    3. FINAL OUTPUT MUST BE PURE JSON. No conversation, no preamble.
    
    Output: JSON with summary, per-file scores, components, issues, and recommendations.
  expected_output: >
    JSON report: {summary: {total, critical, major, minor, avg_score}, file_scores: [{file_path, freshness_score, severity, confidence, components, issues}]}
  agent: freshness_scorer
  context:
    - audit_task


suggestion_task:
  description: >
    You are generating FIX SUGGESTIONS ONLY for documentation issues in {project_path}.

    IMPORTANT:
    - Do not edit repository files.
    - Do not call apply_fix.
    - Provide before/after snippets and unified diffs as suggestions only.

    OUTPUT STABILITY (STRICT):
    - Return the exact same report structure on every run.
    - Use this exact heading sequence only:
      1) # Documentation Freshness Audit Report
      2) ## Executive Summary
      3) ## File-by-File Scorecard
      4) ## File-by-File Analysis
      5) ## Recommendations
      6) ---
      7) Report generated: <ISO-8601 UTC timestamp>
    - Do not add extra top-level sections.
    - Do not omit any required section.
    - If there are no issues in a file, still include that file with "No issues found" and a preventive recommendation.

    LENGTH REQUIREMENT:
    - Make the report detailed enough for ~10 minutes of reading.
    - Target 1400-2200 words.
    - Executive Summary must include 5-8 bullets.
    - For each file include at least:
      - 2 issue bullets (or explicit no-issue note)
      - 3 reasoning bullets
      - 2 recommendations
    - Overall Recommendations section must contain at least 8 numbered items.

    Produce a clean Markdown report in this exact structure:

    # Documentation Freshness Audit Report

    ## Executive Summary
    - Project files analyzed: **<N>**
    - Average freshness score: **<avg>**
    - Severity counts: critical **<n>**, major **<n>**, minor **<n>**

    ## File‑by‑File Scorecard
    - **SORTING RULE**: THIS TABLE MUST BE SORTED ALPHABETICALLY BY FILE PATH (A-Z). NO EXCEPTIONS.
    - Use normalized relative paths (e.g., api.py, utils.py).
    | File | Doc Type | Freshness | Severity | Confidence |
    |---|---|---|---|---|
    | (Use EXACT values from freshness_scorer_task output) |

    ## File-by-File Analysis
    - **ORDER**: ANALYZE FILES IN THE SAME SORTED ORDER AS THE SCORECARD.
    For each file include:
    - doc type, freshness score, severity, confidence
    - issue list (description, location, expected, actual, impact)
    - suggested fix before/after snippets
    - unified diff
    - short reasoning and recommendation

    End with:
    ---
    Report generated: <ISO-8601 UTC timestamp>
  expected_output: >
    A clean Markdown report titled "# Documentation Freshness Audit Report" containing:
    an executive summary, a file-by-file scorecard table, detailed analysis with
    suggested before/after snippets and unified diffs, and recommendations.
    The output must follow a deterministic heading order and be long-form
    (target 1400-2200 words, approximately 10 minutes reading time).
    No project files are modified.
  agent: fix_suggester
  context:
    - audit_task
    - freshness_scorer_task
  human_input: true