audit_task:
  description: >
    Audit the documentation in the project at {project_path}. Documentation files (README, SRS, etc.) are under {docs_path}.
    Steps:
    1. FIRST, use the 'List Files Tool' to see all files in {project_path} (code) and in {docs_path} (documentation).
    2. Then, for each relevant Python file under {project_path}, use 'Docstring Signature Auditor' to check docstrings.
    3. Use 'README Structure Auditor' on the README in {docs_path} (or {project_path} if same).
    4. Use 'API Implementation Auditor' if routes are detected in {project_path}.
    5. Use 'Code Comment Auditor' to extract context from code in {project_path}.
    6. Use 'SRS Parser' to parse local SRS markdown files in {docs_path}.
    7. Use 'git_analyzer' to get file modification history and last-changed dates.
  expected_output: >
    A detailed audit report of all discrepancies found, grouped by file path.
  agent: documentation_auditor


freshness_scorer_task:
  description: >
    Given the comparison data from the Code & Documentation Analyzer, score each 
        file's documentation freshness ,classify severity and confidence.
       
        YOUR TASKS:
        
        1.Parse Analyzer Output: Extract all comparison data for each file
        
        2.Evaluate Scoring Components(assign values 0-1 for each):
        
           A. FRESHNESS FACTORS:
           - structural_match: Do parameters, return types, and endpoints align?
             * 1.0 = Perfect match
             * 0.5 = Some mismatches
             * 0.0 = Major structural differences
           
           - semantic_accuracy: Does documentation describe actual behavior?
             * 1.0 = Description perfectly matches implementation
             * 0.5 = Partially accurate, some outdated info
             * 0.0 = Description completely wrong
           
           - recency_factor: How recent is the documentation vs code?
             * 1.0 = Updated within days of code change
             * 0.5 = Updated within months
             * 0.0 = Not updated in 6+ months after code change
           
           - completeness: Are all code elements documented?
             * 1.0 = All functions/classes/methods documented
             * 0.5 = 50% documented
             * 0.0 = Minimal or no documentation
        
           B. CONFIDENCE FACTORS:
           - mismatch_clarity: How obvious are the discrepancies?
             * 1.0 = Clear, objective mismatch (e.g., param count difference)
             * 0.5 = Requires some interpretation
             * 0.0 = Highly subjective or unclear
           
           - code_complexity: How complex is the code?
             * 1.0 = Very complex, hard to understand
             * 0.5 = Moderate complexity
             * 0.0 = Simple, straightforward
           
           - doc_structure_quality: How well-structured is the documentation?
             * 1.0 = Well-structured, easy to parse
             * 0.5 = Acceptable structure
             * 0.0 = Poor or no structure
        
        3.Use freshness_scorer Tool: Call the tool with all 7 parameters to get:
           - freshness_score (0-100)
           - severity (critical/major/minor)
           - confidence (0-1)
           - detailed breakdown
        
        4.Document Specific Issues: For each file, list problems with:
           - Clear description of the issue
           - Location (line number or section)
           - Expected value (what code shows)
           - Actual value (what doc says)
           - Impact explanation (why this matters)
        
        5.Provide Recommendations: Actionable steps to improve documentation freshness
        
          SCORING INTERPRETATION:
        
          Freshness Score:
            - < 50% → CRITICAL: Immediate attention required, documentation unusable
            - 50-74% → MAJOR: Should be addressed soon, significant issues
            - >= 75% → MINOR: Low priority, minor improvements needed
        
          Confidence Level:
            - > 0.8 → HIGH: Very certain about the assessment
            - 0.5-0.8 → MEDIUM: Reasonably confident
            - < 0.5 → LOW: Requires manual review
        
        EXAMPLES:
        
        Example 1 - Clear Parameter Mismatch:
        ```
        freshness_scorer(
            structural_match=0.25,      # 1 of 4 params documented
            semantic_accuracy=0.70,     # Description mostly accurate
            recency_factor=0.30,        # 6 months outdated
            completeness=0.60,          # 3 of 5 methods documented
            mismatch_clarity=0.95,      # Very clear param count difference
            code_complexity=0.40,       # Moderate complexity
            doc_structure_quality=0.80  # Well-structured docs
        )
        → Score: ~42% (critical), Confidence: 0.82 (high)
        ```
        
        Example 2 - Semantic Drift:
        ```
        freshness_scorer(
            structural_match=0.90,      # Structure looks good
            semantic_accuracy=0.40,     # Behavior changed, docs didn't
            recency_factor=0.20,        # Very outdated
            completeness=0.85,          # Well documented
            mismatch_clarity=0.45,      # Requires interpretation
            code_complexity=0.70,       # Complex code
            doc_structure_quality=0.75  # Good structure
        )
        → Score: ~61% (major), Confidence: 0.48 (low - needs review)
        ```
        
        **OUTPUT REQUIREMENTS:**
        Produce a comprehensive JSON report for each file analyzed.
  expected_output: >
    A JSON array containing detailed freshness assessments for each file:
       
       [
         {
           "file_path": "src/api/user_service.py",
           "doc_type": "inline_docstring",
           "freshness_score": 45.5,
           "severity": "critical",
           "confidence": 0.92,
           "score_breakdown": {
             "structural_match": 0.30,
             "semantic_accuracy": 0.50,
             "recency_factor": 0.60,
             "completeness": 0.40
           },
           "confidence_factors": {
             "mismatch_clarity": 0.95,
             "code_complexity": 0.40,
             "doc_structure_quality": 0.80
           },
           "issues": [
             {
               "description": "Parameter mismatch in create_user method",
               "location": "line 45, create_user() docstring",
               "expected": "4 parameters: username, email, password, role",
               "actual": "2 parameters: username, email",
               "impact": "Developers will be unaware of required 'password' and 'role' parameters, leading to incorrect API usage"
             },
             {
               "description": "Return type documentation outdated",
               "location": "line 52",
               "expected": "Returns: Dict[str, Any] with user_id, timestamp, status",
               "actual": "Returns: int (user_id only)",
               "impact": "Consumers expecting only user_id will break when accessing timestamp/status fields"
             }
           ],
           
         },
         {
           "file_path": "src/utils/validators.py",
           "doc_type": "inline_docstring",
           "freshness_score": 82.3,
           "severity": "minor",
           "confidence": 0.88,
           "score_breakdown": {
             "structural_match": 0.90,
             "semantic_accuracy": 0.85,
             "recency_factor": 0.70,
             "completeness": 0.80
           },
           "confidence_factors": {
             "mismatch_clarity": 0.85,
             "code_complexity": 0.25,
             "doc_structure_quality": 0.90
           },
           "issues": [
             {
               "description": "Minor example outdated in validate_email()",
               "location": "line 23, docstring example",
               "expected": "Example should show new domain validation",
               "actual": "Example uses old validation pattern",
               "impact": "Low - developers may use outdated pattern in examples"
             }
           ],
           
         }
       ]
       
       Include all files from analyzer output with complete scoring details.

  agent: freshness_scorer
  context:
    - audit_task


suggestion_task:
  description: >
    You are generating FIX SUGGESTIONS ONLY for documentation issues in {project_path} / {docs_path}.

    IMPORTANT:
    - Do not edit repository files.
    - Do not call apply_fix.
    - Provide before/after snippets and unified diffs as suggestions only.

    OUTPUT STABILITY (STRICT):
    - Return the exact same report structure on every run.
    - Use this exact heading sequence only:
      1) # Documentation Freshness Audit Report
      2) ## Executive Summary
      3) ## File-by-File Scorecard
      4) ## File-by-File Analysis
      5) ## Recommendations
      6) ---
      7) Report generated: <ISO-8601 UTC timestamp>
    - Do not add extra top-level sections.
    - Do not omit any required section.
    - If there are no issues in a file, still include that file with "No issues found" and a preventive recommendation.

    LENGTH REQUIREMENT:
    - Make the report detailed enough for ~10 minutes of reading.
    - Target 1400-2200 words.
    - Executive Summary must include 5-8 bullets.
    - For each file include at least:
      - 2 issue bullets (or explicit no-issue note)
      - 3 reasoning bullets
      - 2 recommendations
    - Overall Recommendations section must contain at least 8 numbered items.

    Produce a clean Markdown report in this exact structure:

    # Documentation Freshness Audit Report

    ## Executive Summary
    - Project files analyzed: **<N>**
    - Average freshness score: **<avg>**
    - Severity counts: critical **<n>**, major **<n>**, minor **<n>**

    ## File-by-File Scorecard
    | File | Doc Type | Freshness | Severity | Confidence |
    |---|---:|---:|---:|---:|

    ## File-by-File Analysis
    For each file include:
    - doc type, freshness score, severity, confidence
    - issue list (description, location, expected, actual, impact)
    - suggested fix before/after snippets
    - unified diff
    - short reasoning and recommendation

    End with:
    ---
    Report generated: <ISO-8601 UTC timestamp>
  expected_output: >
    A clean Markdown report titled "# Documentation Freshness Audit Report" containing:
    an executive summary, a file-by-file scorecard table, detailed analysis with
    suggested before/after snippets and unified diffs, and recommendations.
    The output must follow a deterministic heading order and be long-form
    (target 1400-2200 words, approximately 10 minutes reading time).
    No project files are modified.
  agent: fix_suggester
  # human_input: true
