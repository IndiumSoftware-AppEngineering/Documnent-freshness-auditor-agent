audit_task:
  description: >
    Audit documentation freshness for {project_path} by following these EXACT steps:
    
    1. DISCOVERY: Use 'list_files' to get a complete list of all files in the project.
    
    2. CHECKLIST: Create a mental or scratchpad checklist of all discovered files.
    
    3. ITERATIVE ANALYSIS (LOOP): For EACH file in your checklist, perform the appropriate checks, ALWAYS passing 'project_root={project_path}' to EVERY tool:
       - Python Files (.py): 
         * Run 'Docstring Signature Auditor' (passing project_root={project_path}).
         * Run 'Code Comment Auditor' (passing project_root={project_path}).
         * Run 'git_analyzer' (passing project_root={project_path}).
       - Markdown Files (.md):
         * Run 'README Structure Auditor' (passing project_root={project_path}) if it's a README.
         * Run 'SRS Parser' (passing path={project_path}) if it's an SRS.
       - API Specs (yaml/json):
         * Run 'API Implementation Auditor' (passing project_root={project_path}).
    
    4. THOROUGHNESS CHECK: Verify you have called at least one tool for EVERY file listed in step 1.
    
    5. REPORTING:
       - Assemble a JSON object containing a 'files' array.
       - Each entry in 'files' MUST include:
           * 'file': The normalized RELATIVE path (e.g., 'api.py'). NEVER use absolute paths.
           * 'metrics': A dictionary with these EXACT keys (ignore others): 
                - total_functions
                - functions_with_docstrings
                - total_params
                - documented_params
                - critical_issues
                - major_issues
                - minor_issues
                - last_updated_iso
                - doc_type
           * 'issues': The list of findings (can be an empty list if no issues).
       - You MUST include an entry for EVERY single file found in the DISCOVERY step.
       - FINAL OUTPUT MUST BE PURE JSON. No conversation, no preamble.

    Output: JSON array with issues grouped by file, each with severity and description.
  expected_output: >
    JSON array of issues: [{"file": "path", "type": "docstring|readme|api|srs|comment", "line": N, "severity": "critical|major|minor", "description": "...", "metrics": {...}}]
  agent: documentation_auditor


freshness_scorer_task:
  description: >
    Calculate documentation freshness scores (0-100%) and confidence levels 
    EXCLUSIVELY using the 'freshness_scorer' tool for EVERY file identified 
    in the audit_task.
    
    STRICT INSTRUCTIONS:
    1. For each file in the audit output:
       a. Extract all available metrics from the audit task JSON for that file.
       b. CALL the 'freshness_scorer' tool with these arguments for EVERY SINGLE file:
          - file_path: the file's RELATIVE path (e.g., 'api.py')
          - metrics: a structured object containing:
            - doc_type: str (ALLOWED: inline_docstring, readme, api_spec, srs, documentation)
            - total_functions: int
            - functions_with_docstrings: int
            - total_params: int
            - documented_params: int
            - critical_issues: int
            - major_issues: int
            - minor_issues: int
            - last_updated_iso: str (YYYY-MM-DD or empty string)
       c. Use the tool response ONLY.
       d. Capture the EXACT output from the tool.
    2. NEVER estimate any score or severity yourself.
    3. FINAL OUTPUT MUST BE PURE JSON. No conversation, no preamble.
    
    Output: JSON with summary, per-file scores, components, issues, and recommendations.
  expected_output: >
    JSON report: {summary: {total, critical, major, minor, avg_score}, file_scores: [{file_path, freshness_score, severity, confidence, components, issues}]}
  agent: freshness_scorer
  context:
    - audit_task


suggestion_task:
  description: >
    You are generating FIX SUGGESTIONS ONLY for documentation issues in {project_path}.

    ROBOTIC SORTING RULE (URGENT/CRITICAL):
    - You MUST sort the final report (Scorecard AND Analysis) strictly ALPHABETICALLY by file path (A-Z).
    - This is your FIRST priority. If the list is not sorted, the report is a FAILURE.

    IMPORTANT:
    - Do not edit repository files.
    - Do not call apply_fix.
    - Provide before/after snippets and unified diffs as suggestions only.

    OUTPUT STABILITY (STRICT):
    - Return the exact same report structure on every run.
    - Use this exact heading sequence only:
      1) # Documentation Freshness Audit Report
      2) ## Executive Summary
      3) ## File-by-File Scorecard
      4) ## File-by-File Analysis
      5) ## Recommendations
      6) ---
      7) Report generated: <ISO-8601 UTC timestamp>
    - Do not add extra top-level sections.
    - Do not omit any required section.
    - If there are no issues in a file, still include that file with "No issues found" and a preventive recommendation.

    LENGTH REQUIREMENT:
    - Make the report detailed enough for ~10 minutes of reading.
    - Target 1400-2200 words.
    - Executive Summary must include 5-8 bullets.
    - For each file include at least:
      - 2 issue bullets (or explicit no-issue note)
      - 3 reasoning bullets
      - 2 recommendations
    - Overall Recommendations section must contain at least 8 numbered items.

    Produce a clean Markdown report in this exact structure:

    # Documentation Freshness Audit Report

    ## Executive Summary
    - Project files analyzed: **<N>**
    - Average freshness score: **<avg>**
    - Severity counts: critical **<n>**, major **<n>**, minor **<n>**

    ## File‑by‑File Scorecard
    - **SORTING RULE (CRITICAL)**: THIS TABLE MUST BE SORTED ALPHABETICALLY BY FILE PATH (A-Z).
    - Use the standard ASCII order (e.g., api.py comes before utils.py).
    - If you miss this, the report is invalid.
    - Use normalized RELATIVE paths ONLY (e.g., api.py, docs/SRS.md). NEVER use absolute paths starting with /Users/...
    | File | Doc Type | Freshness | Severity | Confidence |
    |---|---|---|---|---|
    | (Use EXACT values from freshness_scorer_task output) |

    ## File-by-File Analysis
    - **ORDER**: ANALYZE FILES IN THE SAME SORTED ORDER AS THE SCORECARD.
    For each file include:
    - doc type, freshness score, severity, confidence
    - issue list (description, location, expected, actual, impact)
    - suggested fix before/after snippets
    - unified diff
    - short reasoning and recommendation

    End with:
    ---
    Report generated: <ISO-8601 UTC timestamp>
  expected_output: >
    A clean Markdown report titled "# Documentation Freshness Audit Report" containing:
    an executive summary, a file-by-file scorecard table, detailed analysis with
    suggested before/after snippets and unified diffs, and recommendations.
    The output must follow a deterministic heading order and be long-form
    (target 1400-2200 words, approximately 10 minutes reading time).
    No project files are modified.
  agent: fix_suggester
  context:
    - audit_task
    - freshness_scorer_task
  human_input: true


  