audit_task:
  description: >
    Audit documentation freshness for {project_path}.
    
    1. List all files and categorize: Python (.py), README, API specs (swagger/openapi), requirements.
    
    2. DOCSTRING CHECKS:
       - Extract docstrings from Python functions
       - Compare parameters in docstring vs function signature
       - Flag: missing params, extra params, wrong return type, missing exceptions
       - For each issue: file, function, line, description, severity (critical/major/minor)
    
    3. README CHECKS:
       - Find version numbers, paths, dependencies, examples
       - Compare against requirements.txt, pyproject.toml, code
       - Flag: version mismatch, deleted paths, wrong examples, outdated config
    
    4. API SPEC CHECKS:
       - Compare swagger/openapi spec with route implementation
       - Flag: unimplemented endpoints, schema mismatch, missing validation, wrong method
    
    5. CODE COMMENTS:
       - Find inline comments describing behavior
       - Check if comments match actual code
       - Flag: misleading comments, outdated TODOs, false claims
    
    6. SRS/REQUIREMENTS:
       - Parse requirements.md or design.md
       - Check if features are implemented
       - Flag: missing features, removed features, unmatched requirement IDs
    
    7. FILE HISTORY:
       - Use git to find when code and docs last changed
       - Flag files where code changed but docs didn't (6+ months)
       
    Every issue must cite specific file content as evidence.
    If evidence is missing, mark issue as "low confidence" instead of omitting it.

    Output: JSON array with issues grouped by file, each with severity and description.
  expected_output: >
    JSON array of issues: [{"file": "path", "type": "docstring|readme|api|srs|comment", "line": N, "severity": "critical|major|minor", "description": "..."}]
  agent: documentation_auditor


freshness_scorer_task:
  description: >
    Score documentation freshness (0-100%) based on audit issues.
    
    For each file with issues:
    
    1. COUNT ISSUES:
       - Count critical, major, minor issues
    
    2. SCORE COMPONENTS (0.0-1.0 each):
       - structural_match: (documented_items / total_items) - e.g., 2 of 4 params = 0.50
       - semantic_accuracy: Does doc match behavior? 1.0=perfect, 0.5=partial, 0.0=wrong
       - recency_factor: Days since last update? 0-7 days=1.0, 7-30=0.8, 30-90=0.6, 90-180=0.4, 180+=0.0
       - completeness: What percentage documented? (0.0-1.0)
       - mismatch_clarity: How obvious? 0.9-1.0=obvious, 0.5-0.7=partial, 0.0-0.4=unclear
       - code_complexity: 0.0-0.3=simple, 0.4-0.7=moderate, 0.8-1.0=complex
       - doc_structure_quality: 0.8-1.0=well-structured, 0.5-0.7=acceptable, 0.0-0.4=poor
    
    3. CALCULATE SCORE:
       freshness = (structural_match×0.20 + semantic_accuracy×0.25 + recency_factor×0.25 + completeness×0.20 + mismatch_clarity×0.10) × 100
       Adjust: subtract 5 per critical, 3 per major, 1 per minor issue
       Final: max(0, min(100, freshness))
    
    4. ASSIGN SEVERITY:
       <40%: CRITICAL, 40-70%: MAJOR, 70-85%: MINOR, >85%: EXCELLENT
    
    5. ASSIGN CONFIDENCE (0.0-1.0):
       confidence = (mismatch_clarity×0.35 + doc_structure×0.25 + (1-complexity/2)×0.20 + semantic_accuracy×0.20)
    
    6. FOR EACH ISSUE LIST:
       {issue_name, location, severity, what_docs_say, what_code_does, why_it_matters, fix_priority}
    
    Output: JSON with summary, per-file scores, components, issues, and recommendations.
  expected_output: >
    JSON report: {summary: {total, critical, major, minor, avg_score}, file_scores: [{file_path, freshness_score, severity, confidence, components, issues}]}
  agent: freshness_scorer
  context:
    - audit_task


suggestion_task:
  description: >
    Generate fix suggestions with diffs and before/after examples.
    
    1. For each CRITICAL issue: Show current docstring/doc → corrected version
    2. For each MAJOR issue: Same - with explanation
    3. For each MINOR issue: Brief fix suggestion
    
    Use unified diff format (@@, +, -) for clarity.
    
    4. Create scorecard table:
       | File | Type | Score | Severity | Issues |
       
    5. Produce markdown report:
       - Executive Summary
       - File-by-File Scorecard (table)
       - Detailed Analysis (before/after for each critical/major issue)
       - Recommendations (prioritized)
    
    Make output copy-paste ready for developers.
  expected_output: >
    Markdown report: "Documentation Freshness Audit Report" with executive summary, scorecard table, before/after fixes with diffs, and action plan
  agent: fix_suggester
  context:
    - audit_task
    - freshness_scorer_task
  human_input: true