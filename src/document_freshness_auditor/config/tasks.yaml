audit_task:
  description: >
    Audit documentation freshness for {project_path}.
    
    1. List all files and categorize: Python (.py), README, API specs (swagger/openapi), requirements.
    
    2. DOCSTRING CHECKS:
       - Extract docstrings from Python functions
       - Compare parameters in docstring vs function signature
       - Flag: missing params, extra params, wrong return type, missing exceptions
       - For each issue: file, function, line, description, severity (critical/major/minor)
    
    3. README CHECKS:
       - Find version numbers, paths, dependencies, examples
       - Compare against requirements.txt, pyproject.toml, code
       - Flag: version mismatch, deleted paths, wrong examples, outdated config
    
    4. API SPEC CHECKS:
       - Compare swagger/openapi spec with route implementation
       - Flag: unimplemented endpoints, schema mismatch, missing validation, wrong method
    
    5. CODE COMMENTS:
       - Find inline comments describing behavior
       - Check if comments match actual code
       - Flag: misleading comments, outdated TODOs, false claims
    
    6. SRS/REQUIREMENTS:
       - Parse requirements.md or design.md
       - Check if features are implemented
       - Flag: missing features, removed features, unmatched requirement IDs
    
    7. FILE HISTORY:
       - Use git to find when code and docs last changed
       - Flag files where code changed but docs didn't (6+ months)
       
    Every issue must cite specific file content as evidence.
    If evidence is missing, mark issue as "low confidence" instead of omitting it.

    Output: JSON array with issues grouped by file, each with severity and description.
  expected_output: >
    JSON array of issues: [{"file": "path", "type": "docstring|readme|api|srs|comment", "line": N, "severity": "critical|major|minor", "description": "..."}]
  agent: documentation_auditor


freshness_scorer_task:
  description: >
    Score documentation freshness (0-100%) based on audit issues.
    
    For each file with issues:
    
    1. COUNT ISSUES:
       - Count critical, major, minor issues
    
    2. SCORE COMPONENTS (0.0-1.0 each):
       - structural_match: (documented_items / total_items) - e.g., 2 of 4 params = 0.50
       - semantic_accuracy: Does doc match behavior? 1.0=perfect, 0.5=partial, 0.0=wrong
       - recency_factor: Days since last update? 0-7 days=1.0, 7-30=0.8, 30-90=0.6, 90-180=0.4, 180+=0.0
       - completeness: What percentage documented? (0.0-1.0)
       - mismatch_clarity: How obvious? 0.9-1.0=obvious, 0.5-0.7=partial, 0.0-0.4=unclear
       - code_complexity: 0.0-0.3=simple, 0.4-0.7=moderate, 0.8-1.0=complex
       - doc_structure_quality: 0.8-1.0=well-structured, 0.5-0.7=acceptable, 0.0-0.4=poor
    
    3. CALCULATE SCORE:
       freshness = (structural_match×0.20 + semantic_accuracy×0.25 + recency_factor×0.25 + completeness×0.20 + mismatch_clarity×0.10) × 100
       Adjust: subtract 5 per critical, 3 per major, 1 per minor issue
       Final: max(0, min(100, freshness))
    
    4. ASSIGN SEVERITY:
       <40%: CRITICAL, 40-70%: MAJOR, 70-85%: MINOR, >85%: EXCELLENT
    
    5. ASSIGN CONFIDENCE (0.0-1.0):
       confidence = (mismatch_clarity×0.35 + doc_structure×0.25 + (1-complexity/2)×0.20 + semantic_accuracy×0.20)
    
    6. FOR EACH ISSUE LIST:
       {issue_name, location, severity, what_docs_say, what_code_does, why_it_matters, fix_priority}
    
    Output: JSON with summary, per-file scores, components, issues, and recommendations.
  expected_output: >
    JSON report: {summary: {total, critical, major, minor, avg_score}, file_scores: [{file_path, freshness_score, severity, confidence, components, issues}]}
  agent: freshness_scorer
  context:
    - audit_task


suggestion_task:
  description: >
    You are generating FIX SUGGESTIONS ONLY for documentation issues in {project_path}.

    IMPORTANT:
    - Do not edit repository files.
    - Do not call apply_fix.
    - Provide before/after snippets and unified diffs as suggestions only.

    OUTPUT STABILITY (STRICT):
    - Return the exact same report structure on every run.
    - Use this exact heading sequence only:
      1) # Documentation Freshness Audit Report
      2) ## Executive Summary
      3) ## File-by-File Scorecard
      4) ## File-by-File Analysis
      5) ## Recommendations
      6) ---
      7) Report generated: <ISO-8601 UTC timestamp>
    - Do not add extra top-level sections.
    - Do not omit any required section.
    - If there are no issues in a file, still include that file with "No issues found" and a preventive recommendation.

    LENGTH REQUIREMENT:
    - Make the report detailed enough for ~10 minutes of reading.
    - Target 1400-2200 words.
    - Executive Summary must include 5-8 bullets.
    - For each file include at least:
      - 2 issue bullets (or explicit no-issue note)
      - 3 reasoning bullets
      - 2 recommendations
    - Overall Recommendations section must contain at least 8 numbered items.

    Produce a clean Markdown report in this exact structure:

    # Documentation Freshness Audit Report

    ## Executive Summary
    - Project files analyzed: **<N>**
    - Average freshness score: **<avg>**
    - Severity counts: critical **<n>**, major **<n>**, minor **<n>**

    ## File-by-File Scorecard
    | File | Doc Type | Freshness | Severity | Confidence |
    |---|---:|---:|---:|---:|

    ## File-by-File Analysis
    For each file include:
    - doc type, freshness score, severity, confidence
    - issue list (description, location, expected, actual, impact)
    - suggested fix before/after snippets
    - unified diff
    - short reasoning and recommendation

    End with:
    ---
    Report generated: <ISO-8601 UTC timestamp>
  expected_output: >
    A clean Markdown report titled "# Documentation Freshness Audit Report" containing:
    an executive summary, a file-by-file scorecard table, detailed analysis with
    suggested before/after snippets and unified diffs, and recommendations.
    The output must follow a deterministic heading order and be long-form
    (target 1400-2200 words, approximately 10 minutes reading time).
    No project files are modified.
  agent: fix_suggester
  context:
    - audit_task
    - freshness_scorer_task
  human_input: true